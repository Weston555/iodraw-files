```mermaid
Understood. I will create two types of theoretical framework diagrams to illustrate your code:

1. **Flowchart Format** – to show the procedural flow and data dependencies throughout the execution pipeline.
2. **Layered Architecture Format** – to highlight the modular structure and the integration of portfolio optimization, LSTM predictions, and PPO-based reinforcement learning.

Both diagrams will include key mathematical formulations such as the Sharpe ratio, portfolio variance, and PPO loss function. I will emphasize how LSTM outputs feed into the portfolio selection and how that informs the PPO environment and training pipeline.

I’ll get started and let you know when it’s ready.

# Multi-Asset Portfolio Optimization and PPO Trading Framework

This framework integrates **multi-asset portfolio optimization** with a **reinforcement learning (PPO) trading agent**. We first optimize asset weights for maximum Sharpe ratio or minimum volatility, then use the selected portfolio in a custom trading environment for PPO training. Below, we provide two diagrams outlining the process: a **Flowchart** (step-by-step logic) and a **Layered Architecture** (modular design of components). Key formulas (Sharpe ratio, portfolio variance, PPO loss) are included in **LaTeX** for clarity, and relevant file directories (`data/`, `results/`, `results/predictions/`) are referenced where appropriate.

## Flowchart: End-to-End Process

1. **Data Loading & Preprocessing:** Load historical market data for each asset from CSV files in the `data/` directory ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=for%20file%20in%20os,not%20in%20df.columns)). Ensure each DataFrame has required columns (Date, **Close** price, technical indicators like ROC, RSI, Std_dev). If an asset file lacks a *PredictedReturn* column (LSTM model prediction), initialize it to 0.0 ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=,fillna%280.0%29%20stock_data%5Bticker%5D%20%3D%20df)). All data is sorted by date and prepared for analysis.

2. **Incorporate LSTM Predictions:** For each asset, attempt to load its LSTM predicted returns from `results/predictions/` (e.g. a pickled file or CSV) and merge it with the price data on Date ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=for%20ticker%20in%20tickers%3A%20df,except%20Exception%3A%20try%3A%20pred_df)). Rename the prediction column to **PredictedReturn** and fill missing values with 0.0 ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=pred_col%20%3D%20,fillna%280.0%29%20stock_data%5Bticker%5D%20%3D%20df.copy)). This adds an extra feature (forecasted return) to the dataset for use in portfolio decisions and the RL environment.

3. **Portfolio Optimization (Training Period):** Using the first ~80% of the data as the training period, compute daily returns for each asset and then annualize the statistics (multiplying mean returns and covariance by 252 trading days) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%B9%B4%E5%8C%96%E5%A4%84%E7%90%86%EF%BC%9A252%20%E4%B8%AA%E4%BA%A4%E6%98%93%E6%97%A5%20annualized_mu%20%3D%20returns_matrix,252)). Define the **Sharpe ratio** \( S = \frac{R_p - R_f}{\sigma_p} \) and **portfolio variance** \( \sigma_p^2 = w^T \Sigma w \) as objectives:
   - *Maximum Sharpe Portfolio:* Set up an optimization to maximize Sharpe (equivalently, minimize the negative Sharpe). With risk-free rate \(R_f = 0\), the negative Sharpe objective is \( -\frac{R_p - 0}{\sigma_p} \). This is implemented and passed to SciPy’s optimizer ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%AE%9A%E4%B9%89%E8%B4%9F%E5%A4%8F%E6%99%AE%E6%AF%94%E7%8E%87%E5%87%BD%E6%95%B0%20def%20neg_sharpe_ratio,rf_rate%29%20%2F%20portf_vol)). Constraints enforce that weights sum to 1 and are within [0,1] ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E4%BC%98%E5%8C%96%E7%BA%A6%E6%9D%9F%E4%B8%8E%E8%BE%B9%E7%95%8C%20constraints%20%3D%20%7B,0)). Solving this yields `max_sharpe_weights` and its performance (expected return, volatility, Sharpe) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E6%9C%80%E5%A4%A7%E5%A4%8F%E6%99%AE%E6%AF%94%E7%8E%87%E7%BB%84%E5%90%88%EF%BC%88Portfolio%201%EF%BC%89%20max_sharpe_res%20%3D%20sco,np.sqrt%28np.dot%28max_sharpe_weights%2C%20np.dot%28annualized_cov)).
   - *Minimum Volatility Portfolio:* Similarly, optimize to minimize \( \sigma_p^2 = w^T \Sigma w \) (portfolio variance) under the same constraints ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%AE%9A%E4%B9%89%E7%BB%84%E5%90%88%E6%96%B9%E5%B7%AE%E5%87%BD%E6%95%B0%20def%20portfolio_variance,dot%28cov_mat%2C%20w)). This gives `min_vol_weights` and associated performance metrics ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=min_vol_res%20%3D%20sco,np.sqrt%28np.dot%28min_vol_weights%2C%20np.dot%28annualized_cov%2C%20min_vol_weights)).  
   *Result:* Two candidate portfolios are identified – one maximizing Sharpe ratio and one minimizing risk. A summary of each (weights and stats) is output for review.

4. **Portfolio Selection & Asset Filtering:** Prompt the user to choose one of the two optimized portfolios (Portfolio 1: Max Sharpe or Portfolio 2: Min Vol) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=portfolio_choice%20%3D%20input%28,selected_portfolio_weights_full%20%3D%20min_vol_weights%20else)). Based on the choice, take the corresponding weight vector. Apply a small weight **threshold** (e.g. 0.1%) to filter out negligible allocations ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E6%A0%B9%E6%8D%AE%E6%9D%83%E9%87%8D%E9%98%88%E5%80%BC%EF%BC%88%E4%BE%8B%E5%A6%82%E5%A4%A7%E4%BA%8E%200.1%25%EF%BC%89%E4%BF%9D%E7%95%99%E7%94%A8%E4%BA%8E%E4%BA%A4%E6%98%93%E7%9A%84%E8%82%A1%E7%A5%A8%20threshold%20%3D%201e,i%5D%20for%20i%20in%20selected_indices)). Only assets with weight above this threshold are retained for trading, and their weights are re-normalized to 100% ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=selected_indices%20%3D%20list,%E5%BD%92%E4%B8%80%E5%8C%96)). This yields the **selected asset list** and initial weight distribution for the RL trading strategy. *(If all weights are below threshold, it defaults to using all assets.)* The selected portfolio’s composition is printed (asset tickers and their percentage weights).

5. **Construct Multi-Factor Trading Environment:** Using the chosen assets, build a custom Gym environment to simulate multi-asset trading:
   - Prepare arrays for each key factor over time: asset prices, daily returns, RSI values, volatility (Std_dev as a proxy for risk), and the **PredictedReturn** (LSTM forecast) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=for%20j%2C%20ticker%20in%20enumerate,values.astype%28np.float32%29%20pred_arr%5B%3A%2C%20j%5D%20%3D%20get_1d_series%28df)). Additionally, generate a **Prophet trend signal** for each day using Facebook Prophet: if the next-day forecast price > today’s, set trend = +1, else -1 ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=future_test%20%3D%20pd.DataFrame%28%7B,0)). All these factors form the state inputs.
   - **State Representation:** At each time step, the state includes, for each asset, a window of the last 30 days of returns plus the current values of RSI, volatility, predicted return, and trend signal ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=past_returns%20%3D%20past_returns,astype%28np.float32%29%20features.append%28asset_features%29%20state%20%3D%20np.concatenate%28features%29.astype%28np.float32)). This yields a rich **feature vector** capturing momentum (recent returns), mean-reversion or overbought/oversold status (RSI), risk (volatility), expected return (prediction), and trend.
   - **Action & Transition:** The action is a vector of new portfolio weights (continuous values 0.0 to 1.0 for each asset) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=self.observation_space%20%3D%20spaces.Box%28low%3D,float32)). When the agent takes an action, the environment normalizes it to ensure a valid allocation (non-negative weights summing to 1) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=def%20step,current_weights%20%3D%20weights)), then applies it for the next day. The portfolio’s value evolves as the weighted sum of asset returns for that day is applied to the current portfolio value ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=daily_returns%20%3D%20self.returns,return%20next_state%2C%20portfolio_return%2C%20done%2C)). The environment then advances by one day (time step).
   - **Reward Structure:** The reward at each step is the **portfolio return** achieved that day (i.e. the increase in portfolio value as a fraction of the previous value) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=daily_returns%20%3D%20self.returns,return%20next_state%2C%20portfolio_return%2C%20done%2C)). This encourages the agent to maximize growth of the portfolio. The episode runs through the time series (e.g. remaining 20% of data for testing) until the end is reached.
   - **Initial Conditions:** The environment is initialized with the **selected portfolio weights** and a starting portfolio value of 1.0 (100%) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=self,n_assets)). This means the agent begins trading from the optimized portfolio allocation.

6. **PPO Agent Training:** Initialize a PPO agent (using Stable-Baselines3’s `PPO` with an MLP policy) to interact with the training environment ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=model%20%3D%20PPO%28,ppo_multi_asset_model)). The agent learns to adjust asset allocations by trial-and-error to maximize cumulative reward (portfolio growth). Key aspects of PPO training:
   - **Clipped Surrogate Objective:** PPO uses a loss function that prevents too large a policy update. The objective (for each time step \(t\)) is:  

     $$L^{\text{CLIP}}(\theta) = \hat{\mathbb{E}}_t \Big[ \min\!\Big( r_t(\theta)\,\hat{A}_t,\; \text{clip}\big(r_t(\theta),\, 1-\epsilon,\, 1+\epsilon\big)\,\hat{A}_t \Big) \Big]$$  

     Here \(r_t(\theta)\) is the ratio of the new policy probability to the old policy probability for the taken action, and \(\hat{A}_t\) is the advantage estimate. The $\min$ operator with the clipped ratio $\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)$ ensures the loss penalizes changes that move \(r_t\) outside the range $[1-\epsilon,\;1+\epsilon]$, keeping the new policy close to the old ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=match%20at%20L402%20action%2C%20_,portfolio_value)). This stabilizes training.
   - **Training Process:** The agent runs for many iterations (e.g. 100,000 time steps) on the training data, gradually improving its policy. PPO being an on-policy algorithm will sample trajectories, compute advantages, and update the policy network using the above loss repeatedly. The result is a learned trading policy that seeks to dynamically rebalance the portfolio for higher returns. The trained model is saved to the `results/ppo_multi_asset` directory for reuse ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=model%20%3D%20PPO%28,ppo_multi_asset_model)).

7. **Simulation & Performance Evaluation:** After training, the learned policy is evaluated on the **test dataset** (the remaining ~20% of data not seen during training). The PPO agent is run in the environment **deterministically** (no exploration noise) to simulate real trading day by day ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=while%20not%20done%3A%20action%2C%20_,for%20stock%2C%20w%20in)). During this simulation:
   - The agent picks an action (portfolio weights) based on the current state each day ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=while%20not%20done%3A%20action%2C%20_,for%20stock%2C%20w%20in)), and the environment responds with the next state and the realized reward. We record each step’s date, portfolio value, and chosen weights in a log for analysis ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E8%AE%B0%E5%BD%95%E6%AF%8F%E4%B8%80%E6%AD%A5%E4%BA%A4%E6%98%93%EF%BC%9A%E5%8C%85%E6%8B%AC%E4%BA%A4%E6%98%93%E6%97%A5%E3%80%81%E7%BB%84%E5%90%88%E5%B8%82%E5%80%BC%E3%80%81%E5%BD%93%E5%89%8D%E6%9D%83%E9%87%8D%20transaction_log%20%3D%20%5B%5D%20transaction_log.append%28,)).
   - A **Buy-and-Hold baseline** is also computed for comparison: an equal-weight portfolio held throughout the test period (without rebalancing) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%9F%BA%E5%87%86%E7%AD%96%E7%95%A5%EF%BC%9A%E4%B9%B0%E5%85%A5%E5%B9%B6%E6%8C%81%E6%9C%89%EF%BC%88%E7%AD%89%E6%9D%83%E9%85%8D%E7%BD%AE%EF%BC%8C%E6%97%A0%E5%86%8D%E5%B9%B3%E8%A1%A1%EF%BC%89%20test_start_idx%20%3D%20train_size%20last_idx,idx%2C%20%3A%5D%29%29%20baseline_vals.append%28port_val)). This baseline helps benchmark the RL strategy’s performance against a passive strategy.
   - **Metrics Calculation:** At the end of the test run, key performance metrics are calculated:  
     - **Total Return:** The percentage gain of the portfolio from start to finish of the test period (e.g. final portfolio value ÷ initial value – 1).  
     - **Sharpe Ratio:** Using daily returns of the strategy, compute \( \text{Sharpe} = \frac{\text{mean}(r_{\text{daily}})}{\text{std}(r_{\text{daily}})} \times \sqrt{252} \) for annualized Sharpe ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=annual_factor%20%3D%20np,0%20else%200.0)). This is done for both the PPO strategy and the buy-and-hold baseline.  
     - **Maximum Drawdown:** Compute the greatest peak-to-trough drop in portfolio value during the period ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=def%20calculate_max_drawdown%28values%29%3A%20peak%20%3D%20,max_dd%20%3D%20drawdown%20return%20max_dd)). This measures risk by showing the worst-case loss from a high point.  
   - The results (final return %, Sharpe ratio, max drawdown) for both the PPO strategy and the baseline are reported ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=print%28,max_drawdown_base%3A.2)). Typically, a successful RL strategy would have a higher Sharpe and return, with a comparable or lower drawdown, versus the baseline.
   - All transaction records are saved to a CSV (e.g. `PPO_Portfolio_transactions.csv` in `results/ppo_multi_asset/transactions/`) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=transactions_dir%20%3D%20os.path.join%28results_dir%2C%20,index%3DFalse)). A plot of the cumulative portfolio value over time is generated comparing PPO vs. Buy-and-Hold, annotated with the initial and final portfolio allocations ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E7%94%9F%E6%88%90%E5%88%9D%E5%A7%8B%E4%B8%8E%E6%9C%80%E7%BB%88%E7%BB%84%E5%90%88%E6%9D%83%E9%87%8D%E7%9A%84%E6%B3%A8%E8%AE%B0%EF%BC%88%E8%8B%B1%E6%96%87%E6%A0%BC%E5%BC%8F%EF%BC%8C%E4%B8%8D%E5%90%AB%E4%B8%AD%E6%96%87%EF%BC%89%20initial_weights_annot%20%3D%20,5)) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=plt.plot%28test_dates%2C%20rl_values%2C%20label%3D,trades_summary.png)). A summary text file is also written with the chosen portfolio and performance stats ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=results_txt%20%3D%20os.path.join%28results_dir%2C%20,1%5D%3A.2f%7D%2C%20Sharpe%20Ratio%3A%20%7Bsharpe_rl%3A.2f%7D%2C%20Max)) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=Drawdown%3A%20%7Bmax_drawdown_rl%3A.2,n)).

## Layered Architecture Overview

The system can be viewed in a **layered architecture**, with each layer responsible for specific functionality and interacting with adjacent layers. From bottom (data input) to top (agent and outputs), the layers are:

- **Data Processing Layer:** This bottom layer handles ingestion and preparation of data. It reads raw *market data* (CSV files in `data/`) and *model predictions* (`results/predictions/` files) for multiple assets. It cleans and merges these inputs into a unified format ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=for%20ticker%20in%20tickers%3A%20df,except%20Exception%3A%20try%3A%20pred_df)) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=pred_col%20%3D%20,fillna%280.0%29%20stock_data%5Bticker%5D%20%3D%20df.copy)), computes derived features (daily returns, technical indicators like RSI and volatility if not already present), and ensures the dataset is complete (filling missing values, sorting by date). The output of this layer is a structured dataset for each asset, ready for analysis and use in optimization or the trading environment.

- **Portfolio Optimization Layer:** This layer takes the processed historical data and performs quantitative portfolio analytics. It calculates key statistics: average returns and the covariance matrix of asset returns (annualized) from the training period ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%B9%B4%E5%8C%96%E5%A4%84%E7%90%86%EF%BC%9A252%20%E4%B8%AA%E4%BA%A4%E6%98%93%E6%97%A5%20annualized_mu%20%3D%20returns_matrix,252)). Using these, it formulates optimization problems to find optimal weight allocations:
  - *Maximum Sharpe Ratio Portfolio:* maximize \(S = \frac{R_p - R_f}{\sigma_p}\) by solving for weights \(w\) (with \(R_f=0\) here) that maximize expected return for a given risk ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%AE%9A%E4%B9%89%E8%B4%9F%E5%A4%8F%E6%99%AE%E6%AF%94%E7%8E%87%E5%87%BD%E6%95%B0%20def%20neg_sharpe_ratio,rf_rate%29%20%2F%20portf_vol)). In practice, this is done by minimizing the negative Sharpe objective under weight sum and bound constraints using SciPy SLSQP ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E6%9C%80%E5%A4%A7%E5%A4%8F%E6%99%AE%E6%AF%94%E7%8E%87%E7%BB%84%E5%90%88%EF%BC%88Portfolio%201%EF%BC%89%20max_sharpe_res%20%3D%20sco,np.sqrt%28np.dot%28max_sharpe_weights%2C%20np.dot%28annualized_cov)).
  - *Minimum Volatility Portfolio:* minimize \( \sigma_p^2 = w^T \Sigma w \) to find the lowest-risk portfolio ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E5%AE%9A%E4%B9%89%E7%BB%84%E5%90%88%E6%96%B9%E5%B7%AE%E5%87%BD%E6%95%B0%20def%20portfolio_variance,dot%28cov_mat%2C%20w)) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=min_vol_res%20%3D%20sco,np.sqrt%28np.dot%28min_vol_weights%2C%20np.dot%28annualized_cov%2C%20min_vol_weights)) (also under \( \sum w_i=1, w_i\ge0 \)).  
  This layer outputs two sets of optimal weights (and their performance metrics). A user decision is made to pick one of these portfolios as the starting point for trading (this decision could be seen as part of this layer or a small interface above it). It also establishes which assets will be included (filtering out assets with near-zero weights) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E6%A0%B9%E6%8D%AE%E6%9D%83%E9%87%8D%E9%98%88%E5%80%BC%EF%BC%88%E4%BE%8B%E5%A6%82%E5%A4%A7%E4%BA%8E%200.1%25%EF%BC%89%E4%BF%9D%E7%95%99%E7%94%A8%E4%BA%8E%E4%BA%A4%E6%98%93%E7%9A%84%E8%82%A1%E7%A5%A8%20threshold%20%3D%201e,i%5D%20for%20i%20in%20selected_indices)), thus defining the asset universe for the next layer.

- **Reinforcement Learning Environment Layer:** This middle layer provides a **simulation environment** for trading the selected portfolio assets. It encapsulates the market dynamics and state representation for the RL agent:
  - **State Space:** For each asset in the selected portfolio, the state includes recent historical data and current signals. Concretely, the environment compiles a vector of length 34 per asset: the last 30 days of returns, plus 4 current indicators – RSI, volatility (Std_dev), predicted return, and Prophet trend signal ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=past_returns%20%3D%20past_returns,astype%28np.float32%29%20features.append%28asset_features%29%20state%20%3D%20np.concatenate%28features%29.astype%28np.float32)). The full state is the concatenation of these vectors for all assets, giving the agent a holistic view of the multi-asset market conditions at time $t$.
  - **Action Space:** The action is defined as a weight allocation across the $N$ assets (a continuous vector of length $N$ where each element \(a_i \in [0,1]\) and $\sum_i a_i = 1$) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=self.observation_space%20%3D%20spaces.Box%28low%3D,float32)). The agent’s action represents rebalancing the portfolio to new weights. The environment enforces validity by clipping negative values to 0 and re-normalizing the weights to sum to 1 ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=def%20step,current_weights%20%3D%20weights)).
  - **Market Dynamics & Reward:** The environment advances in **daily time steps**. Given an action (new weights) on day $t$, the portfolio return on the next day $t+1$ is computed as \( r_{p,t+1} = \sum_{i=1}^N w_i \cdot r_{i,t+1} \), where \(r_{i,t+1}\) is asset $i$’s return on that day ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=daily_returns%20%3D%20self.returns,return%20next_state%2C%20portfolio_return%2C%20done%2C)). The portfolio value is updated accordingly, and this **portfolio return** serves as the reward signal to the agent. The next state reflects the new time window (rolling forward one day). This layer thus provides a realistic multi-asset **trading environment** with multifactor observations and reward feedback for the agent. It also uses the initially chosen weights as the starting allocation and begins the episode with portfolio value 1.0.
  - **Episode Termination:** The environment is configured to run through the available data (e.g., until the end of the test set). An episode ends when the last day is reached ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=return%20self._get_state%28%29%2C%200.0%2C%20True%2C%20,return%20next_state%2C%20portfolio_return%2C%20done%2C)). The design allows for splitting data into a training period (for agent learning) and a testing period (for performance evaluation) by instantiating separate environment instances with different date ranges ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=%E6%9E%84%E9%80%A0%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%EF%BC%8880,1)).

- **PPO Agent Layer:** The top layer consists of the **Proximal Policy Optimization agent** that interacts with the environment to learn and execute an optimal trading strategy:
  - **Training Phase:** The PPO agent uses the training environment (with historical data up to the 80% mark) to learn an allocation policy. It employs an actor-critic neural network (MLP policy) to predict actions (asset weight vectors) given the state, and is trained by optimizing the clipped surrogate loss (as described above) using observed rewards and calculated advantages. Over many iterations, the agent adjusts its policy network parameters $\theta$ to maximize expected reward. The training process leverages the PPO algorithm’s stable improvement guarantee via clipping ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=match%20at%20L402%20action%2C%20_,portfolio_value)), eventually producing a policy that ostensibly captures the relationships between the input factors (RSI, trends, etc.) and future returns. The trained model is saved (`ppo_multi_asset_model.zip`) for later use ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=model%20%3D%20PPO%28,ppo_multi_asset_model)).
  - **Execution & Evaluation:** Once trained, the agent is deployed on the testing environment (the remaining 20% of data it has not seen) to evaluate performance. In this phase, the agent operates **deterministically**, using the learned policy to decide daily reallocations without further learning ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=while%20not%20done%3A%20action%2C%20_,for%20stock%2C%20w%20in)). The sequence of actions and resulting portfolio values is recorded. Simultaneously, this layer computes performance metrics – total return, Sharpe ratio, and max drawdown – for the agent’s strategy vs. a benchmark (equal-weight buy-and-hold) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=annual_factor%20%3D%20np,0%20else%200.0)) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=def%20calculate_max_drawdown%28values%29%3A%20peak%20%3D%20,max_dd%20%3D%20drawdown%20return%20max_dd)). The PPO agent layer thus produces the final outputs: the trading strategy’s performance statistics and logs of decisions. These results are saved to the `results/ppo_multi_asset/` directory (including a CSV of trades and charts of equity curves) for analysis and visualization ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=df_transaction%20%3D%20pd.DataFrame%28transaction_log%29%20df_transaction.to_csv%28os.path.join%28transactions_dir%2C%20,index%3DFalse)) ([4.3_PPO.py](file://file-VxqXU4KuvBETnpvVBToEBr#:~:text=cumulative_plot_path%20%3D%20os.path.join%28results_dir%2C%20,close)).

Each layer builds upon the previous one in a hierarchical fashion: the Data layer feeds the Optimization layer, which defines the starting portfolio for the Environment layer, which in turn is used by the PPO Agent layer for learning and decision-making. This modular design cleanly separates data handling, financial optimization, environment simulation, and learning algorithm, making the system easier to understand and extend.
```